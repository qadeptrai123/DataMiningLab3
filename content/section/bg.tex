\section{Kiến thức cơ sở}

Gọi $P \subset \mathbb{R}^d$ và $k$ lần lượt là tập dữ liệu và số lượng cụm. Gọi $m$ là kích thước của tập dữ liệu. Đối với hai điểm $p, q \in \mathbb{R}^d$ bất kỳ, ký hiệu $\delta(p, q)$ và $\delta^2(p, q)$ lần lượt là khoảng cách và bình phương khoảng cách giữa chúng. Cho một điểm $p \in \mathbb{R}^d$ và một tập các tâm $C = \{c_1, c_2, \dots, c_k\}$, gọi $\delta(p, C) = \min_{c \in C} \delta(p, c)$ là khoảng cách từ $p$ đến tâm gần nhất trong $C$. 

% BEGIN PB
Để ý có thể có nhiều cách phân cụm tối ưu nhưng ở đây tác giả chọn 1 cách cố định để phân tích.
% END PB

Gọi $C^* = \{c^*_1, \dots, c^*_k\}$ và $P(C^*) = \{P^*_1, \dots, P^*_k\}$ là tập các tâm tối ưu và phân hoạch phân cụm tối ưu tương ứng. Mỗi tâm tối ưu $c^*_i \in C^*$ được biểu diễn bởi $d$ tọa độ, tức là $c^*_i = (c^*_{i1}, c^*_{i2}, \dots, c^*_{id})$. Chi phí phân cụm của tập $P$ đối với tập tâm $C$ được định nghĩa là:
\begin{equation}
    \delta^2(P, C) = \sum_{x \in P} \delta^2(x, C)
\end{equation}

Cho một tập hợp $L(P) = \{P_1, P_2, \dots, P_k\}$ đóng vai trò là bộ dự đoán, gọi $Q_i = P_i \cap P^*_i$ là tập các điểm dữ liệu trong cụm dự đoán $P_i$ thuộc cụm tối ưu $P^*_i$. Gọi tọa độ chiếu của các điểm dữ liệu trong $P_i$ và $Q_i$ lên chiều thứ $j$ lần lượt là $P_{ij}$ và $Q_{ij}$. Gọi $P^*_{ij}$ là tọa độ chiếu của các điểm trong $P^*_i$ lên chiều thứ $j$. Gọi $m_i$ và $m$ lần lượt là kích thước của $P_i$ và $P$. Với một tập điểm dữ liệu $V \subset \mathbb{R}^d$, gọi $\overline{V}$ là tâm hình học của tập $V$. Gọi $P(j)$ là tọa độ chiếu của toàn bộ các điểm trong $P$ lên chiều thứ $j$. Gọi $\Delta_{max}$ là tỷ lệ chiều tối đa của các điểm dữ liệu được chiếu, được xác định bởi:
\begin{equation}
    \Delta_{max} = \max_{1 \leq j \leq d} \frac{\max_{x,y \in P(j)} \delta(x,y)}{\min_{x,y \in P(j), x \neq y} \delta(x,y)}
\end{equation}

% BEGIN PB
Cụm từ ``Aspect Ratio'' được tác giả đề cập khi dịch về tiếng Việt để
quen thuộc nhất thì chúng em sẽ gọi là \textbf{tỷ lệ khung hình}. 
Về bản chất thì cũng chỉ là tỉ lệ giữa khoảng cách lớn nhất của 2 điểm 
và khoảng cách nhỏ nhất của 2 điểm.
% END PB

Với một số nguyên dương $t$, gọi $[t]$ là tập hợp các số nguyên từ 1 đến $t$.

\textbf{Bài toán $k$-means hỗ trợ học:} Cho tập dữ liệu $P \subset \mathbb{R}^d$ gồm $m$ điểm, gọi $C^*$ và $P(C^*) = \{P^*_1, P^*_2, \dots, P^*_k\}$ lần lượt là một lời giải tối ưu và phân hoạch tương ứng. Trong thiết lập có hỗ trợ học, giả định rằng có quyền truy cập vào một bộ dự đoán dưới dạng phân hoạch nhãn $L(P) = \{P_1, P_2, \dots, P_k\}$ được tham số hóa bởi tỷ lệ lỗi nhãn $\alpha \in [0, 1)$, thỏa mãn điều kiện $|P_i \cap P^*_i| \geq (1-\alpha) \max\{|P_i|, |P^*_i|\}$. Mục tiêu của bài toán là tìm tập $C \subset \mathbb{R}^d$ các tâm sao cho $\delta^2(P, C)$ đạt giá trị nhỏ nhất.

% BEGIN PB
Các hệ quả toán học trong phần này dựa trên tính chất cơ bản của không gian Euclid, trọng tâm $\overline{V}$ là điểm duy nhất tối thiểu hóa tổng bình phương khoảng cách (SSE) tới mọi điểm trong tập $V$. Logic của các bổ đề dưới đây cho phép phân rã chi phí phân cụm thành hai thành phần: chi phí trong cụm (độ liên kết - cohesion) và chi phí do khoảng cách từ tâm dự đoán đến tâm tối ưu.
% END PB

Các bổ đề dưới đây là "dân gian truyền miệng" (folklore) rất phổ biến liên quan bài toán $k$-means clustering

\begin{lemma}
Cho tập $X \subset \mathbb{R}^d$ có kích thước $m$ và một điểm dữ liệu bất kỳ $c \in \mathbb{R}^d$, ta luôn có:
\begin{equation}
    \delta^2(X, c) = \delta^2(X, \overline{X}) + m \cdot \delta^2(c, \overline{X})
\end{equation}
\cite{Arthur2007}
\end{lemma}

% BEGIN PB
Bổ đề trên xuất phát từ quan sát trọng tâm $\overline{P_i}$ của các cụm dự đoán không đủ là nghiệm của bài toán. Vì dự đoán không đúng, có thể tồn tại 1 số điểm trong $P_i$ nằm ngoài $P_i^*$. Nếu các điểm trong $P_i \setminus P_i^*$ nằm \textbf{rất xa} $\overline{P_i^*}$, trọng tâm cụm dự đoán sẽ bị lệch tuỳ ý dẫn đến chi phí tăng cụm tăng lên tuỳ ý. 
% END PB

\begin{lemma}
Cho tập $J \subset \mathbb{R}$, gọi $J_1 \subseteq J$ với $|J_1| \geq (1-\zeta)|J|$, trong đó $0 \leq \zeta < 1$. Khi đó, mối liên hệ giữa chi phí của tập con và tập tổng thể được chặn bởi:
\begin{equation}
    \delta^2(\overline{J}, \overline{J_1}) \leq \frac{\zeta}{(1-\zeta)|J|} \delta^2(J, \overline{J})
\end{equation}
\cite{Nguyen2022}
\end{lemma}

% BEGIN PB
Bổ đề trên cũng đúng với $J \subset \mathbb{R}^d$, mặc dù tác giả chỉ ghi trên $\mathbb{R}$, xem chứng minh ở . % TODO \ref

Bổ đề trên xuất phát từ quan sát liên hệ chi phí và kích thước tập con của cụm tối ưu. Ta muốn tìm $Q_i = P_i \cap P_i^*$ và lấy $\overline{Q_i}$ làm đáp án cho cụm $i$, điều này tự nhiên xuất phát từ dữ kiện $Q_i$ của bài toán có hỗ trợ học. 
\begin{align*}
|Q_i| &\geq (1-\alpha) \max\{|P_i|, |P^*_i|\} \geq (1 - \alpha) |P_i^*| \\
\Rightarrow |P_i^* \setminus Q_i| &\leq \alpha m_i^*
\end{align*}

Dùng bổ đề trên, ta có chặn trên chi phí phân cụm: 
\begin{align*}
\delta^2(P_i^*, \overline{Q_i}) &=\delta^2(P_i^*, \overline{P_i^*}) +  m_i^* \delta^2(\overline{P_i^*}, \overline{Q_i}) \text{   (bổ đề 1)} \\
 &\leq \delta^2(P_i^*, \overline{P_i^*}) + m_i^* \frac{\alpha}{1 - \alpha}  \frac{\delta^2(P_i^*, \overline{P_i^*})}{m_i^*} \text{    (bổ đề 2)}\\
 &= \left(1 + \frac{\alpha}{1 - \alpha} \right) \delta^2(P_i^*, \overline{P_i^*})
\end{align*}

Như vậy ta có xấp xỉ $\left(1 + \frac{\alpha}{1 - \alpha} \right)$ cho 1 cụm và cũng như cho bài toán. Cũng chính là chặn dưới và lí do xuất hiện của nó trong tỷ lệ xấp xỉ trong \ref{tab:theoretical_comparison}.

Khó khăn là $Q_i$ chưa biết, vì vậy 3 thuật toán chính dưới đây tập trung vào việc loại bỏ các điểm outlier trong $P_i$, tìm một trọng tâm gần $\overline{Q_i}$, như vậy đồng thời giảm được khoảng cách đến $\overline{P_i^*}$ và chi phí. 
% END PB


\begin{lemma}
Cho tập $X \subset \mathbb{R}^d$ và một giá trị $\alpha \in (0, 1]$, gọi $X' = \arg \min_{X'' \subseteq X, |X''| = \alpha|X|} \delta^2(X'', \overline{X''})$. Khi đó, ta có:
\begin{equation}
    \delta^2(X', \overline{X'}) \leq \alpha \cdot \delta^2(X, \overline{X})
\end{equation}
\cite{Nguyen2022}
\end{lemma}

% BEGIN PB
\subsection{Các công cụ (toán)}

% Bernoulli ineq

% Cauchy

% Relaxed triangle ineq
\begin{theorem2}[Bất đẳng thức tam giác nới lỏng]
\label{thm:relaxed_triangle}
Với mọi số thực $a, b \in \mathbb{R}$ và một tham số dương $\lambda > 0$, bất đẳng thức sau luôn thỏa mãn:
\[ (a + b)^2 \leq \left(1 + \frac{1}{\lambda}\right)a^2 + (1 + \lambda)b^2 \]
Trong không gian vector $\mathbb{R}^d$ với chuẩn Euclid $\| \cdot \|$, bất đẳng thức này tương đương với:
\[ \|u + v\|^2 \leq \left(1 + \frac{1}{\lambda}\right)\|u\|^2 + (1 + \lambda)\|v\|^2 \]
\end{theorem2}

\begin{proof}
Ở đây nhóm em chứng minh cho 1 chiều, còn lại cũng tương tự. Ta bắt đầu bằng việc khai triển vế trái của bất đẳng thức:
\[ (a + b)^2 = a^2 + 2ab + b^2 \]

Ta áp dụng bất đẳng thức AM-GM. Với hai số thực dương $x, y$, ta luôn có $x^2 + y^2 \geq 2xy$.
Chọn $x = \frac{a}{\sqrt{\lambda}}$ và $y = b\sqrt{\lambda}$. Khi đó:
\[ \left( \frac{a}{\sqrt{\lambda}} \right)^2 + (b\sqrt{\lambda})^2 \geq 2 \left( \frac{a}{\sqrt{\lambda}} \right) (b\sqrt{\lambda}) \]

\[ \frac{a^2}{\lambda} + \lambda b^2 \geq 2ab \]

Thay thế chặn trên của $2ab$ vào khai triển ban đầu của $(a + b)^2$:
\begin{align*}
    (a + b)^2 &= a^2 + 2ab + b^2 \\
    &\leq a^2 + \left( \frac{a^2}{\lambda} + \lambda b^2 \right) + b^2 \\
    &= \left( a^2 + \frac{a^2}{\lambda} \right) + (b^2 + \lambda b^2) \\
    &= \left( 1 + \frac{1}{\lambda} \right)a^2 + (1 + \lambda)b^2
\end{align*}

\end{proof}
% END PB

\subsection{Các thuật toán con}

\subsubsection{Chọn phần tử hạng thứ $i$ trong dãy}

Hay chọn trung vị trong $O(m)$.
Thay vì $O(m \log{m})$